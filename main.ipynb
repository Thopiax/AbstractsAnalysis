{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Papers based on contents of their abstracts\n",
    "\n",
    "Author: Rafael Ballestiero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "import os, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize']=[50,30]\n",
    "plt.rcParams['font.size']=22\n",
    "plt.rcParams['font.weight']='bold'\n",
    "plt.rcParams['axes.titlesize'] = 28\n",
    "plt.rcParams['axes.labelsize'] = 24\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Construct preprocessed abstracts with custom filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "from gensim.parsing import preprocessing\n",
    "from gensim.parsing.preprocessing import preprocess_string, strip_tags, strip_punctuation, \\\n",
    "                                         strip_multiple_whitespaces, strip_numeric, remove_stopwords, strip_short, stem_text\n",
    "\n",
    "from gensim.utils import has_pattern\n",
    "\n",
    "import pattern.en as en\n",
    "\n",
    "assert has_pattern()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/text_allyears.csv\", header=0, index_col=0).dropna(subset=[\"Abstract\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviations = [\"pss\", \"iot\"]\n",
    "def lemmatize(s):\n",
    "    return \" \".join([en.lemma(w) if w not in abbreviations else w for w in s.split()])\n",
    "\n",
    "cp1252_pattern = re.compile(u\"“|”|’|‘|—|\\?\")\n",
    "def strip_cp1252_punctuation(s):\n",
    "    return re.sub(cp1252_pattern, \" \", s)\n",
    "\n",
    "first_exclusion_common_terms = [\"service\", \"innovation\", \"design\", \"customer\", \"services\", \"research\", \"study\", \"paper\"]\n",
    "second_exclusion_common_terms = [\"service\", \"services\", \"research\", \"study\", \"paper\", \"result\", \"based\", \"literature\", \"article\", \"focus\"]\n",
    "def remove_common_terms(s, exclusion_terms=second_exclusion_common_terms):\n",
    "    return \" \".join([w for w in s.split() if w not in exclusion_terms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abstract_preprocessing(df, name=\"Abstract\"):\n",
    "    return df[\"Abstract\"].apply(str).apply(preprocess_string, filters=[\n",
    "        lambda x: x.lower(),\n",
    "        strip_tags,\n",
    "        strip_cp1252_punctuation,\n",
    "        strip_punctuation, \n",
    "        strip_multiple_whitespaces, \n",
    "        strip_numeric, \n",
    "        remove_stopwords, \n",
    "        strip_short,\n",
    "        lemmatize,\n",
    "        remove_common_terms\n",
    "    ]).reset_index(drop=True).rename(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_abstracts = abstract_preprocessing(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import scipy.cluster.hierarchy as shc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agglomerative_cluster(X, n_clusters):\n",
    "    return shc.fcluster(shc.linkage(X, method='ward'), n_clusters, criterion='maxclust') - 1\n",
    "\n",
    "def k_means_cluster(X, n_clusters, random_state=0):\n",
    "    km_model = KMeans(n_clusters=n_clusters, random_state=random_state, n_jobs=-1)\n",
    "    km_model.fit(X)\n",
    "    return km_model.labels_\n",
    "\n",
    "def cluster_algo_name(func):\n",
    "    if func == agglomerative_cluster:\n",
    "        return \"agglomerative\"\n",
    "    elif func == k_means_cluster:\n",
    "        return \"k_means\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CH/DB score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "def univariate_method_evaluation(X, name, n_clusters_limit, score_metric, score_metric_name, agglomerative):\n",
    "    assert n_clusters_limit >= 2\n",
    "    \n",
    "    k_range = range(2, n_clusters_limit)\n",
    "    \n",
    "    plt.title(f'{score_metric_name} Scores (max_k={n_clusters_limit})')\n",
    "    \n",
    "    if agglomerative:\n",
    "        X_n = spatial.distance.squareform(X)\n",
    "        cluster_algo = agglomerative_cluster\n",
    "    else:\n",
    "        X_n = X.toarray()\n",
    "        cluster_algo = k_means_cluster\n",
    "\n",
    "    scores = []\n",
    "    for k in k_range:\n",
    "        cluster_labels = cluster_algo(X, k)\n",
    "\n",
    "        scores.append(score_metric(X_n, cluster_labels))\n",
    "\n",
    "    plt.plot(k_range, scores, label=name)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.savefig(f'plots/{name}/{score_metric_name}_scores.pdf')\n",
    "    plt.show()\n",
    "    \n",
    "def calinski_harabasz_evaluation(X, name, n_clusters_limit, agglomerative=False):\n",
    "    univariate_method_evaluation(X, name, n_clusters_limit, calinski_harabasz_score, \"CalinskiHarabasz\", agglomerative)\n",
    "    \n",
    "def davies_bouldin_evaluation(X, name, n_clusters_limit, agglomerative=False):\n",
    "    univariate_method_evaluation(X, name, n_clusters_limit, davies_bouldin_score, \"DaviesBouldin\", agglomerative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "def silhouette_evaluation(X, n_clusters, labels, name, cluster_algo=k_means_cluster, config_name=\"default\", squareform=False):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    cluster_labels = cluster_algo(X, n_clusters)\n",
    "    \n",
    "    if squareform:\n",
    "        X = spatial.distance.squareform(X)\n",
    "    \n",
    "    silhouette_avg = silhouette_score(X, cluster_labels, sample_size=None)\n",
    "    samples = silhouette_samples(X, cluster_labels)\n",
    "    \n",
    "    max_silhouette_score = np.max(samples)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(0, n_clusters):\n",
    "        cluster_silhouette_scores = samples[cluster_labels == i]\n",
    "        cluster_silhouette_scores.sort()\n",
    "\n",
    "        cluster_size = cluster_silhouette_scores.shape[0]\n",
    "        y_upper = y_lower + cluster_size\n",
    "\n",
    "        ax.fill_betweenx(np.arange(y_lower, y_upper), 0, cluster_silhouette_scores)\n",
    "\n",
    "        ax.text(-0.1 * max_silhouette_score, y_lower + 0.5 * cluster_size, str(i))\n",
    "\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    plt.title(f'Silhouette Graph (k={n_clusters}) - {name} - {config_name}')\n",
    "\n",
    "    ax.set_yticks([])\n",
    "    ax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "    plt.text(silhouette_avg + 0.01,20,f'silhouette_avg={np.round(silhouette_avg, 4)}')\n",
    "    plt.savefig(f'plots/{name}/{n_clusters}/silhouette_{config_name}.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "def tsne_evaluation(X, n_clusters, name, random_state=0, config_name=\"default\", **kwargs):\n",
    "    labels = k_means_cluster(X, n_clusters)\n",
    "    X_truncated = TruncatedSVD(n_components=n_clusters, random_state=random_state).fit_transform(X)\n",
    "    X_repr = TSNE(n_components=2, random_state=random_state, **kwargs).fit_transform(X_truncated)\n",
    "\n",
    "    fig = plt.subplot()\n",
    "    sns.scatterplot(X_repr[:, 0], X_repr[:, 1], s=1000, hue=labels, palette=\"Set3\", legend=\"full\")\n",
    "    plt.title(f'TSNE 2-d Representation (k={n_clusters}) - {name} - {config_name}')\n",
    "\n",
    "    plt.savefig(f'plots/{name}/{n_clusters}/tsne_{config_name}.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Themes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import keywords\n",
    "\n",
    "def stemmed_keyword_mean_score(keywords):\n",
    "    stemmed_scores = {}\n",
    "    stemmed2kwd = {}\n",
    "    \n",
    "    for kwd, score in keywords:\n",
    "        stemmed_kwd = stem_text(stem_text(kwd))\n",
    "        \n",
    "        if stemmed_kwd not in stemmed_scores:\n",
    "            stemmed2kwd[stemmed_kwd] = kwd\n",
    "            stemmed_scores[stemmed_kwd] = []\n",
    "\n",
    "        stemmed_scores[stemmed_kwd].append(score)\n",
    "        \n",
    "    stemmed_scores = {stemmed2kwd[k]: np.mean(v) for (k, v) in stemmed_scores.items()} \n",
    "            \n",
    "    return sorted(stemmed_scores.items(), key=(lambda x: (x[1], x[0])), reverse=True)\n",
    "\n",
    "def text_rank_keyword_scores(clusters, word_count=6, debug=False, abstracts=preprocessed_abstracts, **kwds):\n",
    "    result = []\n",
    "    \n",
    "    scored_keywords = abstracts.apply(\" \".join)\\\n",
    "                               .apply(lambda s: re.sub(u\"–\", \" \", s))\\\n",
    "                               .groupby(clusters)\\\n",
    "                               .apply(\". \".join)\\\n",
    "                               .apply(lambda x: keywords(x, scores=True, **kwds))\n",
    "    \n",
    "\n",
    "    for group, kw_list in scored_keywords.iteritems():\n",
    "        stemmed_keywords_seen = set([])\n",
    "        \n",
    "        for keyword, score in kw_list:\n",
    "            if len(stemmed_keywords_seen) == word_count:\n",
    "                break\n",
    "                \n",
    "            stemmed_keyword = stem_text(stem_text(keyword))\n",
    "            \n",
    "            if debug:\n",
    "                print(\", \".join([keyword, stemmed_keyword, str(score), str(stemmed_keywords_seen)]))\n",
    "            \n",
    "            if stemmed_keyword not in stemmed_keywords_seen:\n",
    "                stemmed_keywords_seen.add(stemmed_keyword)\n",
    "                result.append((group, keyword, score))\n",
    "            \n",
    "    return pd.DataFrame(result, columns=[\"cluster\", \"keyword\", \"score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-idf Transformer Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.sklearn_api import TfIdfTransformer\n",
    "\n",
    "def sum_tfidf_scores(dct, corpus, n=None):\n",
    "    total_score = {}\n",
    "    \n",
    "    for document in corpus:\n",
    "        sorted_doc = sorted(document, key=(lambda x: (x[1], x[0])), reverse=True)\n",
    "        for kwd_id, tfidf_score in sorted_doc[:n]:\n",
    "            kwd = dct[kwd_id]\n",
    "            \n",
    "            if kwd not in total_score:\n",
    "                total_score[kwd] = 0\n",
    "                \n",
    "            total_score[kwd] += tfidf_score\n",
    "            \n",
    "    return sorted(total_score.items(), key=(lambda x: (x[1], x[0])), reverse=True)\n",
    "\n",
    "def tfidf_transformer_keyword_scores(clusters, word_count=6, debug=False, exclude_words=[], abstracts=preprocessed_abstracts, **kwds):\n",
    "    dct = corpora.Dictionary(abstracts)\n",
    "    model = TfIdfTransformer(dictionary=dct)\n",
    "\n",
    "    # train model on all documents\n",
    "    all_docs_corpus = abstracts.apply(dct.doc2bow).tolist()\n",
    "    model.fit(all_docs_corpus)\n",
    "\n",
    "    # create corpus per cluster\n",
    "    cluster_corpus = abstracts.groupby(clusters).apply(lambda x: [dct.doc2bow(abstract) for abstract in x])\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for cluster_id, corpus in cluster_corpus.items():\n",
    "        tfidf_corpus = model.transform(corpus)\n",
    "\n",
    "        words_in_cluster = 0\n",
    "        for keyword, score in sum_tfidf_scores(dct, tfidf_corpus):\n",
    "            if keyword in exclude_words: continue\n",
    "            result.append((cluster_id, keyword, score))\n",
    "            words_in_cluster += 1\n",
    "            \n",
    "            if words_in_cluster == word_count:\n",
    "                break\n",
    "\n",
    "    return pd.DataFrame(result, columns=[\"cluster\", \"keyword\", \"score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-idf Vectorizer Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def sorted_keyword_scores(model):\n",
    "    return sorted(zip(model.get_feature_names(), model.idf_), key=lambda x: (x[1], x[0]))\n",
    "\n",
    "tfidf_vectorizer_keyword_scores_cache = {}\n",
    "\n",
    "def tfidf_vectorizer_keyword_scores(clusters, word_count=6, abstracts=preprocessed_abstracts):\n",
    "    result = []\n",
    "    \n",
    "    abstract_strings = abstracts.apply(\" \".join)\n",
    "    \n",
    "    if abstracts.name not in tfidf_vectorizer_keyword_scores_cache:\n",
    "        model = TfidfVectorizer(tokenizer=None, ngram_range=(1, 2))\n",
    "        model.fit(abstract_strings.values)\n",
    "        \n",
    "        tfidf_vectorizer_keyword_scores_cache[abstracts.name] = model\n",
    "    else:\n",
    "        model = tfidf_vectorizer_keyword_scores_cache[abstracts.name]\n",
    "    \n",
    "    for cluster_id, cluster_idx in abstracts.groupby(clusters).groups.items():\n",
    "        abstracts = abstract_strings.loc[cluster_idx]\n",
    "        \n",
    "        abstracts_scores = model.transform(abstracts.values)\n",
    "        abstracts_scores_df = pd.DataFrame.sparse.from_spmatrix(abstracts_scores, columns=model.get_feature_names())\n",
    "        \n",
    "        sorted_keyword_scores = abstracts_scores_df.mul(model.idf_).sum(0).sort_values(ascending=False)\n",
    "        \n",
    "        for keyword, score in sorted_keyword_scores.iloc[:word_count].iteritems():\n",
    "            result.append((cluster_id, keyword, score))\n",
    "        \n",
    "        \n",
    "    return pd.DataFrame(result, columns=[\"cluster\", \"keyword\", \"score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_scores_algo_name(func):\n",
    "    if func == text_rank_keyword_scores:\n",
    "        return \"text_rank\"\n",
    "    elif func == tfidf_transformer_keyword_scores:\n",
    "        return \"tfidf_transformer\"\n",
    "    elif func == tfidf_vectorizer_keyword_scores:\n",
    "        return \"tfidf_vectorizer\"\n",
    "    \n",
    "def plot_keyword_scores(X_scores, n_clusters, name, keyword_algo, config_name):\n",
    "    fig, axes = plt.subplots(nrows=n_clusters)\n",
    "        \n",
    "    fig.suptitle(f\"Cluster Themes (k={n_clusters}) - {name} - {keyword_scores_algo_name(keyword_algo)} - {config_name}\", fontsize=35)\n",
    "    fig.set_figheight(n_clusters * 5)\n",
    "    \n",
    "    for i, ax in enumerate(axes):\n",
    "        cluster_num = i\n",
    "        \n",
    "        sns.barplot(\n",
    "            x='keyword',  \n",
    "            y='score',  \n",
    "            data=X_scores[X_scores['cluster'] == cluster_num],\n",
    "            ax=ax\n",
    "        )\n",
    "\n",
    "        ax.set_title(f\"cluster={cluster_num}\")\n",
    "        ax.set_xlabel(None)\n",
    "        ax.tick_params(axis='x', labelsize=40)\n",
    "        \n",
    "    fig.savefig(f'plots/{name}/{n_clusters}/keywords_{keyword_scores_algo_name(keyword_algo)}_{config_name}.pdf', format='pdf')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def keyword_evaluation(X, n_clusters, name, cluster_algo=k_means_cluster, keyword_algo=tfidf_transformer_keyword_scores, config_name=\"default\", **kwds):\n",
    "    cluster_labels = cluster_algo(X, n_clusters)\n",
    "    X_scores = keyword_algo(cluster_labels, **kwds)\n",
    "    \n",
    "    plot_keyword_scores(X_scores, n_clusters, name, keyword_algo, config_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfFileMerger\n",
    "\n",
    "def merge_graphs(basename, pdfname, configname, n_cluster_limit=20):\n",
    "    merger = PdfFileMerger()\n",
    "    \n",
    "    for i in range(2, n_cluster_limit):\n",
    "        pdf_path = f\"./plots/{basename}/{i}/{pdfname}_{configname}.pdf\"\n",
    "        merger.append(pdf_path)\n",
    "\n",
    "    merger.write(f\"./plots/{basename}/all_{pdfname}_{configname}.pdf\")\n",
    "    merger.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from nltk import word_tokenize, PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_abstracts_strings = preprocessed_abstracts.apply(\" \".join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    return [stemmer.stem(token) for token in word_tokenize(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_model = TfidfVectorizer(\n",
    "    ngram_range=(1, 1),\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "unigram_scores = unigram_model.fit_transform(preprocessed_abstracts_strings.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2, 22):\n",
    "    if str(i) not in os.listdir(\"./plots/tfidf\"):\n",
    "        os.mkdir(f\"./plots/tfidf/{i}\")\n",
    "        \n",
    "#     silhouette_evaluation(unigram_scores, i, \"tfidf\", config_name=\"only_unigrams\")\n",
    "    tsne_evaluation(unigram_scores, i, \"tfidf\", config_name=\"only_unigrams\")\n",
    "#     keyword_evaluation(unigram_scores, i, \"tfidf\", config_name=\"only_unigrams\")\n",
    "#     keyword_evaluation(unigram_scores, i, \"tfidf\", config_name=\"only_unigrams\", keyword_algo=tfidf_vectorizer_keyword_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_graphs(\"tfidf\", \"silhouette\", \"only_unigrams\", n_cluster_limit=22)\n",
    "merge_graphs(\"tfidf\", \"tsne\", \"only_unigrams\", n_cluster_limit=22)\n",
    "# merge_graphs(\"tfidf\", \"keywords\", \"tfidf_transformer_only_unigrams\", n_cluster_limit=22)\n",
    "# merge_graphs(\"tfidf\", \"keywords\", \"tfidf_vectorizer_only_unigrams\", n_cluster_limit=22)\n",
    "# merge_graphs(\"tfidf\", \"keywords\", \"text_rank_only_unigrams\", n_cluster_limit=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calinski_harabasz_evaluation(unigram_scores, \"tfidf\", 30)\n",
    "davies_bouldin_evaluation(unigram_scores, \"tfidf\", 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigrams and Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_and_bigram_model = TfidfVectorizer(\n",
    "    ngram_range=(1, 2), # search for unigrams and bigrams\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "unigram_and_bigram_scores = unigram_and_bigram_model.fit_transform(preprocessed_abstracts_strings.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2, 20):\n",
    "    if str(i) not in os.listdir(\"./plots/tfidf\"):\n",
    "        os.mkdir(f\"./plots/tfidf/{i}\")\n",
    "#     silhouette_evaluation(unigram_and_bigram_scores, i, \"tfidf\", config_name=\"unigrams_bigrams\")\n",
    "    tsne_evaluation(unigram_and_bigram_scores, i, \"tfidf\", config_name=\"unigrams_bigrams\")\n",
    "#     keyword_evaluation(unigram_and_bigram_scores, i, \"tfidf\", config_name=\"unigrams_bigrams\")\n",
    "#     keyword_evaluation(unigram_and_bigram_scores, i, \"tfidf\", config_name=\"unigrams_bigrams\", keyword_algo=tfidf_vectorizer_keyword_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_graphs(\"tfidf\", \"silhouette\", \"unigrams_bigrams\")\n",
    "merge_graphs(\"tfidf\", \"tsne\", \"unigrams_bigrams\")\n",
    "# merge_graphs(\"tfidf\", \"keywords\", \"tfidf_transformer_unigrams_bigrams\")\n",
    "# merge_graphs(\"tfidf\", \"keywords\", \"tfidf_vectorizer_unigrams_bigrams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multigram_model = TfidfVectorizer(\n",
    "    ngram_range=(1, 5),\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "multigram_scores = multigram_model.fit_transform(preprocessed_abstracts_strings.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2, 20):\n",
    "    if str(i) not in os.listdir(\"./plots/tfidf\"):\n",
    "        os.mkdir(f\"./plots/tfidf/{i}\")\n",
    "#     silhouette_evaluation(multigram_scores, i, \"tfidf\", config_name=\"multigrams\")\n",
    "    tsne_evaluation(multigram_scores, i, \"tfidf\", config_name=\"multigrams\")\n",
    "#     keyword_evaluation(multigram_scores, i, \"tfidf\", config_name=\"multigrams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_graphs(\"tfidf\", \"silhouette\", \"multigrams\")\n",
    "merge_graphs(\"tfidf\", \"tsne\", \"multigrams\")\n",
    "# merge_graphs(\"tfidf\", \"keywords\", \"multigrams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Mover's Distance on GloVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.test.utils import get_tmpfile\n",
    "\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "def convert_glove_2_w2v():\n",
    "    tmp_file = get_tmpfile(\"test_word2vec.txt\")\n",
    "\n",
    "    glove2word2vec(\"./data/glove.6B/glove.6B.50d.txt\", tmp_file)\n",
    "\n",
    "    return KeyedVectors.load_word2vec_format(tmp_file)\n",
    "    \n",
    "# model = convert_glove_2_w2v()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calculate_distance_matrix():\n",
    "    s = preprocessed_abstracts.size\n",
    "    result = np.ndarray((s,s))\n",
    "    \n",
    "    for i, a1 in enumerate(preprocessed_abstracts):\n",
    "        print(f\"({i}/{s}) Calculating distance for abstract...\")\n",
    "        for j, a2 in enumerate(preprocessed_abstracts.iloc[i:]):\n",
    "            distance = model.wmdistance(a1, a2)\n",
    "            result[i][i + j] = distance\n",
    "            result[i + j][i] = distance\n",
    "            print(f\"D_({i}, {i + j})={distance}\")\n",
    "            \n",
    "    return result\n",
    "\n",
    "# abstract_distance_matrix = calculate_distance_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store distance matrix\n",
    "# pd.DataFrame(abstract_distance_matrix).to_csv('data/second_exclusion_distance_matrix.csv')\n",
    "# load distance matrix from memory\n",
    "abstract_distance_matrix = pd.read_csv('data/second_exclusion_distance_matrix.csv', index_col=0).values\n",
    "# create the squareform\n",
    "abstract_df = spatial.distance.squareform(abstract_distance_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dendogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"All Years - Second Exlusion - Dendrogram\")\n",
    "dend = shc.dendrogram(shc.linkage(abstract_df, method='ward'))\n",
    "plt.savefig('plots/wmd_glove/dendrogram.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2, 12):\n",
    "    if str(i) not in os.listdir(\"./plots/wmd_glove\"):\n",
    "        os.mkdir(f\"./plots/wmd_glove/{i}\")\n",
    "    silhouette_evaluation(abstract_df, i, \"wmd_glove\", cluster_algo=agglomerative_cluster, config_name=\"second_exclusion\", squareform=True)\n",
    "    keyword_evaluation(abstract_df, i, \"wmd_glove\", cluster_algo=agglomerative_cluster, config_name=\"second_exclusion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Exclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_graphs(\"wmd_glove\", \"silhouette\", \"second_exclusion\", n_cluster_limit=12)\n",
    "merge_graphs(\"wmd_glove\", \"keywords\", \"tfidf_transformer_second_exclusion\", n_cluster_limit=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calinski_harabasz_evaluation(abstract_df, \"wmd_glove\", 12, agglomerative=True)\n",
    "davies_bouldin_evaluation(abstract_df, \"wmd_glove\", 12, agglomerative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"wmd_glove_3\"] = agglomerative_cluster(abstract_df, 3)\n",
    "df[\"wmd_glove_5\"] = agglomerative_cluster(abstract_df, 5)\n",
    "for i in range(12, 22):\n",
    "    df[f\"tfidf_only_unigrams_{i}\"] = k_means_cluster(unigram_scores, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('results/tfidf_unigrams.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatives\n",
    "\n",
    "### Topic Modeling\n",
    "\n",
    "It seems that there is not enough data available in our dataset (only ~300 paragraphs) to provide interesting results for topic modeling algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaMulticore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct = Dictionary(preprocessed_abstracts)\n",
    "corpus = [dct.doc2bow(abstract) for abstract in preprocessed_abstracts]\n",
    "lda = LdaMulticore(corpus, id2word=dct, num_topics=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### People-Centric papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_centric_df = pd.read_csv(\"./archive/people_centric/data/abstracts.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_centric_abstracts = abstract_preprocessing(people_centric_df, name=\"people_centric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_centric_model = TfidfVectorizer(\n",
    "    ngram_range=(1, 1),\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "people_centric_scores = people_centric_model.fit_transform(people_centric_abstracts.apply(\" \".join).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2, 12):\n",
    "#     if str(i) not in os.listdir(\"./plots/people_centric\"):\n",
    "#         os.mkdir(f\"./plots/people_centric/{i}\")\n",
    "#     silhouette_evaluation(people_centric_scores, i, \"people_centric\")\n",
    "    tsne_evaluation(people_centric_scores, i, \"people_centric\")\n",
    "#     keyword_evaluation(people_centric_scores, i, \"people_centric\", abstracts=people_centric_abstracts)\n",
    "#     keyword_evaluation(people_centric_scores, i, \"people_centric\", abstracts=people_centric_abstracts, keyword_algo=tfidf_vectorizer_keyword_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_graphs(\"people_centric\", \"silhouette\", \"default\", n_cluster_limit=12)\n",
    "merge_graphs(\"people_centric\", \"tsne\", \"default\", n_cluster_limit=12)\n",
    "# merge_graphs(\"people_centric\", \"keywords\", \"tfidf_transformer_default\", n_cluster_limit=12)\n",
    "# merge_graphs(\"people_centric\", \"keywords\", \"tfidf_vectorizer_default\", n_cluster_limit=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_centric_df[\"cluster_6\"] = k_means_cluster(people_centric_scores, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_centric_df.to_csv(\"./results/people_centric_clusters.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
