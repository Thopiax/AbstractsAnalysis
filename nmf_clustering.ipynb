{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-based Paper Clustering\n",
    "\n",
    "Author: Rafael Ballestiero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from fastprogress.fastprogress import master_bar, progress_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize']=[30,20]\n",
    "plt.rcParams['font.size']=22\n",
    "plt.rcParams['font.weight']='bold'\n",
    "plt.rcParams['axes.titlesize'] = 28\n",
    "plt.rcParams['axes.labelsize'] = 24\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Vectorization\n",
    "\n",
    "In order to vectorize any corpus, we must create a tokenizer to extract tokens from each document. Although we could simply use the preprocessed words as they are, it is also interesting to stem them to group together ideas with similar stems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "def build_tokenizer(exceptions):\n",
    "    def _tokenizer(text):\n",
    "        return [stemmer.stem(token) if token not in exceptions else token for token in word_tokenize(text)]\n",
    "    \n",
    "    return _tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_stem2token(iterator, abbreviations):\n",
    "    frequency_map = {}\n",
    "    \n",
    "    def _do_stem2token(agg):\n",
    "        try:\n",
    "            _, text = next(iterator)\n",
    "            \n",
    "            for token in word_tokenize(text):\n",
    "                frequency_map[token] = frequency_map.get(token, 0) + 1\n",
    "                \n",
    "                if token not in abbreviations:\n",
    "                    stem = stemmer.stem(token)\n",
    "                    \n",
    "                    # only add token to aggregator if (i) not present or (ii) token is more popular than current one\n",
    "                    if stem not in agg.keys() or frequency_map[token] > frequency_map[agg[stem]]:\n",
    "                        agg[stem] = token\n",
    "\n",
    "            return _do_stem2token(agg)\n",
    "        except StopIteration:\n",
    "            return agg\n",
    "        \n",
    "    result = {}\n",
    "    return _do_stem2token(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nmf_decomp(X, n_clusters, frobenius=True, random_state=42, **kwargs):\n",
    "    if frobenius:\n",
    "        nmf = NMF(n_components=n_clusters, random_state=random_state, shuffle=True, alpha=.1, **kwargs)\n",
    "    else:\n",
    "        nmf = NMF(n_components=n_clusters, random_state=random_state, shuffle=True, \n",
    "                  beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1, **kwargs)\n",
    "\n",
    "    # since our samples are rows and not columns, H is the document cluster matrix and W is the topic keywords matrix.\n",
    "    H = nmf.fit_transform(X)\n",
    "    W = nmf.components_\n",
    "    \n",
    "    return H, W\n",
    "\n",
    "def nmf_consensus_matrix(X, n_clusters, r, T, random_state=42):\n",
    "    assert 0 < r and r <= 1\n",
    "        \n",
    "    n_documents = X.shape[0]\n",
    "\n",
    "    C = np.zeros((T, n_documents, n_documents))\n",
    "    S = np.zeros((T, n_documents, n_documents))\n",
    "\n",
    "    for t in range(T):\n",
    "        # sample random indices with sampling rate r\n",
    "        doc_idx = np.random.choice(n_documents, size=int(n_documents * r), replace=False)\n",
    "        subset_scores = X[doc_idx, :]\n",
    "\n",
    "        # apply NMF decomposition and get cluster labels\n",
    "        sH, _ = nmf_decomp(subset_scores, n_clusters, random_state=random_state)\n",
    "        subset_labels = sH.argmax(axis=1)\n",
    "\n",
    "        # idx map to get the subset index of the original document index\n",
    "        doc_idx_map = {doc_i: sub_i for (sub_i, doc_i) in enumerate(doc_idx)}\n",
    "\n",
    "        for doc_i in range(n_documents):\n",
    "            for doc_j in range(doc_i, n_documents):\n",
    "                if doc_i in doc_idx and doc_j in doc_idx:\n",
    "                    S[t, doc_i, doc_j] = 1\n",
    "                    # add mirror value\n",
    "                    S[t, doc_j, doc_i] = 1\n",
    "\n",
    "                    # get document's subset ids\n",
    "                    sub_i, sub_j = doc_idx_map[doc_i], doc_idx_map[doc_j]\n",
    "\n",
    "                    if subset_labels[sub_i] == subset_labels[sub_j]:\n",
    "                        C[t, doc_i, doc_j] = 1\n",
    "                        # add mirror value\n",
    "                        C[t, doc_j, doc_i] = 1\n",
    "\n",
    "\n",
    "    confusion_matrix = np.divide(np.sum(C, axis=0), np.sum(S, axis=0))\n",
    "\n",
    "    # handle NaN values due to division by 0 (i.e., no co-occurence between documents in any sample)\n",
    "    return np.nan_to_num(confusion_matrix, 0)\n",
    "\n",
    "def nmf_dispersion(concensus_matrix):\n",
    "    assert concensus_matrix.shape[0] == concensus_matrix.shape[1]\n",
    "    n = concensus_matrix.shape[0]\n",
    "    \n",
    "    return (4 / n**2) * sum(sum((concensus_matrix[i, j] - 0.5)**2 for j in range(n)) for i in range(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from PyPDF2 import PdfFileMerger\n",
    "\n",
    "class CorpusModel:\n",
    "    def __init__(self, corpus_clusterer, name, **parameters):\n",
    "        self.corpus_clusterer = corpus_clusterer\n",
    "        self.name = name\n",
    "        \n",
    "        self.random_state = 42\n",
    "        \n",
    "        self.model = TfidfVectorizer(\n",
    "            tokenizer=corpus_clusterer.tokenizer,\n",
    "            **parameters\n",
    "        )\n",
    "        \n",
    "        self.scores = self.model.fit_transform(corpus_clusterer.corpus)\n",
    "        \n",
    "        self.nmf_decomps = {}\n",
    "        self.dispersion_coefficients = {}\n",
    "        \n",
    "    def format_keyword(self, keyword):\n",
    "        return \" \".join([self.corpus_clusterer.stem2token.get(w, w) for w in keyword.split()])\n",
    "    \n",
    "    def get_nmf_decomp(self, n_clusters, **kwargs):\n",
    "        if n_clusters not in self.nmf_decomps.keys():\n",
    "            self.nmf_decomps[n_clusters] = nmf_decomp(self.scores, n_clusters, random_state=self.random_state, **kwargs)\n",
    "            \n",
    "        return self.nmf_decomps[n_clusters]\n",
    "\n",
    "    def nmf_cluster_labels(self, n_clusters):\n",
    "        H, _ = self.get_nmf_decomp(n_clusters)\n",
    "        \n",
    "        return H.argmax(axis=1)\n",
    "    \n",
    "    def nmf_topic_keywords(self, n_clusters, word_count=10):\n",
    "        _, W = self.get_nmf_decomp(n_clusters)\n",
    "        \n",
    "        result = []\n",
    "        \n",
    "        feature_names = self.model.get_feature_names()\n",
    "\n",
    "        for topic_id, topic in enumerate(W):\n",
    "            top_words = topic.argsort()[::-1][:word_count]\n",
    "\n",
    "            result.append([self.format_keyword(feature_names[i]) for i in top_words])\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def calculate_dispersion_coefficient(self, n_clusters, r=0.8, T=50):\n",
    "        if n_clusters not in self.dispersion_coefficients:\n",
    "            C = nmf_consensus_matrix(self.scores, n_clusters, r, T, random_state=self.random_state)\n",
    "            self.dispersion_coefficients[n_clusters] = nmf_dispersion(C)\n",
    "            \n",
    "        return self.dispersion_coefficients[n_clusters]\n",
    "        \n",
    "class CorpusClusterer:\n",
    "    def __init__(self, name, df, corpus):\n",
    "        self.name = name\n",
    "        self.df = df\n",
    "        self.corpus = corpus\n",
    "        \n",
    "        self.models = {}\n",
    "        \n",
    "        self.maybe_load_abbreviations()\n",
    "        \n",
    "        self.stem2token = build_stem2token(self.corpus.iteritems(), self.abbreviations)\n",
    "        self.tokenizer = build_tokenizer(self.abbreviations)\n",
    "        \n",
    "    def maybe_load_abbreviations(self):\n",
    "        if \"abbreviations.pkl\" in os.listdir(f\"./data/{self.name}\"):\n",
    "            with open(f\"./data/{self.name}/abbreviations.pkl\", \"rb\") as abbfile:\n",
    "                self.abbreviations = pkl.load(abbfile)\n",
    "        else:\n",
    "            self.abbreviations = []\n",
    "                \n",
    "    def add_tfidf_model(self, model_name, **model_params):\n",
    "        self.models[model_name] = CorpusModel(self, model_name, **model_params)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Service Innovation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_innovation_df = pd.read_csv(\"./data/service_innovation/clean.csv\", index_col=0)\n",
    "service_innovation_corpus = pd.read_csv(\"./data/service_innovation/corpus.csv\")['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_clusterer = CorpusClusterer(\"service_innovation\", service_innovation_df, service_innovation_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigrams Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_clusterer.add_tfidf_model(\n",
    "    \"unigram\", \n",
    "    ngram_range=(1, 1), \n",
    "    min_df=2, # must appear in more than one document\n",
    "    max_df=0.99 # must appear in fewer than 99% of documents \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigrams And Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_clusterer.add_tfidf_model(\n",
    "    \"bigram\", \n",
    "    ngram_range=(1, 2), \n",
    "    min_df=3, # must appear in more than one document\n",
    "    max_df=0.95 # must appear in fewer than 99% of documents \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_clusterer.add_tfidf_model(\n",
    "    \"multigram\", \n",
    "    ngram_range=(1, 5), \n",
    "    min_df=3, # must appear in more than one document\n",
    "    max_df=0.95 # must appear in fewer than 99% of documents \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range = range(2, 30)\n",
    "config_range = [\"unigram\", \"bigram\", \"multigram\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb = master_bar(k_range)\n",
    "pb = progress_bar(config_range, parent=mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispersion_df = pd.DataFrame(columns=k_range)\n",
    "\n",
    "for k in mb:\n",
    "    mb.main_bar.comment = f'k={k}'\n",
    "    \n",
    "    max_coeff = 0\n",
    "    max_config = None\n",
    "    \n",
    "    for config in pb:\n",
    "        dispersion_df.loc[config, k] = corpus_clusterer.models[config].calculate_dispersion_coefficient(k)\n",
    "        \n",
    "        if max_coeff < dispersion_df.loc[config, k]:\n",
    "            max_coeff = dispersion_df.loc[config, k]\n",
    "            max_config = config\n",
    "            \n",
    "        mb.child.comment = f'max coeff={max_coeff}; best config={max_config}'\n",
    "        \n",
    "    mb.write(f\"Finished dispersion calculations for k={k}\")\n",
    "        \n",
    "dispersion_df = dispersion_df.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "\n",
    "plt.title(\"NMF Dispersion Coefficients\")\n",
    "\n",
    "sns.lineplot(x=dispersion_df.columns, y=dispersion_df.loc[\"unigram\"], ax=ax, label=\"unigram\")\n",
    "sns.lineplot(x=dispersion_df.columns, y=dispersion_df.loc[\"bigram\"], ax=ax, label=\"bigram\")\n",
    "sns.lineplot(x=dispersion_df.columns, y=dispersion_df.loc[\"multigram\"], ax=ax, label=\"multigram\")\n",
    "\n",
    "ax.set_xlabel(\"k\")\n",
    "ax.set_ylabel(\"p\")\n",
    "\n",
    "plt.savefig('./plots/nmf/dispersion_plot.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels_df = service_innovation_df.copy()\n",
    "\n",
    "for k in k_range:\n",
    "    for config in config_range:\n",
    "        cluster_labels_df[f\"{config}_{k}\"] = corpus_clusterer.models[config].nmf_cluster_labels(k)\n",
    "    \n",
    "cluster_labels_df.to_csv(\"./results/nmf_clusters.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in k_range:\n",
    "    for config in config_range:\n",
    "        pd.DataFrame(corpus_clusterer.models[config].nmf_topic_keywords(k)).to_csv(f\"./results/nmf_topic_keywords_{config}_{k}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
