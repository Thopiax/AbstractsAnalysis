{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Abstracts Analysis\n",
    "\n",
    "By Rafael Ballestiero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Change the PATH variable to the folder where the CSV file is stored.\n",
    "PATH = '/Users/rafa/dv/INSEAD/abstracts'\n",
    "FILENAME = 'papers.csv'\n",
    "\n",
    "os.chdir(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get Abstracts for Paper\n",
    "\n",
    "The first step in this is to parse the DOI links provided in the spreadsheet to retrieve the Abstracts of each of these players. This is done with the help of some HTML parsing libraries and a quick inspection of the page structure to find the HTML elements in which the abstracts appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "paper_urls = pd.read_csv(FILENAME, header=None)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "def retrieve_abstract(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    abstract_section = soup.find(\"div\", {\"class\": \"abstractSection\"})\n",
    "    return abstract_section.p.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have done this we can store this back into a CSV to prevent repeated scraping in further runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading abstracts from CSV\n"
     ]
    }
   ],
   "source": [
    "if 'abstracts.csv' in os.listdir():\n",
    "    print('Reading abstracts from CSV')\n",
    "    df = pd.read_csv('abstracts.csv', index_col=0)\n",
    "else:\n",
    "    abstracts = {}\n",
    "\n",
    "    for i, url in enumerate(paper_urls):\n",
    "        print(f'({i}/{len(paper_urls)})Retrieving abstract for {url}...')\n",
    "        abstracts[url] = retrieve_abstract(url)\n",
    "\n",
    "    df = pd.DataFrame.from_dict(abstracts, orient='index', columns=['Abstract'])\n",
    "\n",
    "    # Ensure integrity of abstracts (none of them are null)\n",
    "    assert (df['Abstract'].isna() == False).all()\n",
    "\n",
    "    df.to_csv('abstracts.csv')\n",
    "    \n",
    "df.columns = ['URL', 'Abstract']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clean Abstracts\n",
    "\n",
    "The text needs to be cleaned before our model is trained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/rafa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess, lemmatize\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Preprocess Abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df['Abstract'].apply(simple_preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Filter Out Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def filter_stopwords(sentence):\n",
    "    result = []\n",
    "    \n",
    "    for word in sentence:\n",
    "        if word not in stopwords:\n",
    "            result.append(word)\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = corpus.apply(filter_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(corpus, size=200)\n",
    "\n",
    "model.save('abstracts_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = model.wv.index2word\n",
    "wvs = model.wv[words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cluster Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, AffinityPropagation"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
